{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "dlfUlOx-bBsT"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # Importamos NumPy para trabajar con matrices\n",
        "import random  # Importamos random para la generación de números aleatorios\n",
        "from tqdm import tqdm  # Importamos tqdm para mostrar barras de progreso en bucles\n",
        "\n",
        "class Board():\n",
        "    def __init__(self):\n",
        "        self.state = np.zeros((6,7))  # Creamos una matriz de 6x7 llena de ceros para representar el tablero del juego 4 en raya\n",
        "\n",
        "    def valid_moves(self):\n",
        "        # Devuelve una lista de tuplas que representan las coordenadas de las casillas vacías en el tablero\n",
        "        return [(i, j) for i in range(6) for j in range(7) if self.state[i, j] == 0]\n",
        "\n",
        "    def update(self, symbol, row, col):\n",
        "        if self.state[row, col] == 0:  # Verifica si la casilla está vacía\n",
        "            self.state[row, col] = symbol  # Coloca el símbolo del jugador en la casilla especificada\n",
        "            return row, col  # Devuelve las coordenadas de la casilla actualizada\n",
        "        else:\n",
        "            raise ValueError(\"Movimiento ilegal !\")  # Si la casilla no está vacía, se lanza un error\n",
        "\n",
        "    def is_game_over(self):\n",
        "        # Comprobamos si alguna fila, columna o diagonal tiene 4 fichas del mismo jugador\n",
        "        for i in range(6):\n",
        "            for j in range(4):\n",
        "                if sum(self.state[i, j:j+4]) == 4 or sum(self.state[j:j+4, i]) == 4:\n",
        "                    return 1  # Devuelve 1 si hay un ganador (jugador 1)\n",
        "                elif sum(self.state[i, j:j+4]) == -4 or sum(self.state[j:j+4, i]) == -4:\n",
        "                    return -1  # Devuelve -1 si hay un ganador (jugador 2)\n",
        "        for i in range(3):\n",
        "            for j in range(4):\n",
        "                if sum([self.state[i+k, j+k] for k in range(4)]) == 4 or sum([self.state[i+k, j+k] for k in range(4)]) == -4:\n",
        "                    return 1\n",
        "                elif sum([self.state[i+k, j+k] for k in range(4)]) == -4 or sum([self.state[i+k, j+k] for k in range(4)]) == -4:\n",
        "                    return -1\n",
        "        for i in range(3):\n",
        "            for j in range(4):\n",
        "                if sum([self.state[i+k, 6-j-k] for k in range(4)]) == 4 or sum([self.state[i+k, 6-j-k] for k in range(4)]) == -4:\n",
        "                    return 1\n",
        "                elif sum([self.state[i+k, 6-j-k] for k in range(4)]) == -4 or sum([self.state[i+k, 6-j-k] for k in range(4)]) == -4:\n",
        "                    return -1\n",
        "        # Si no hay ganador y no hay movimientos válidos restantes, devuelve 0 para indicar un empate\n",
        "        if len(self.valid_moves()) == 0:\n",
        "            return 0\n",
        "        # Si el juego aún no ha terminado, devuelve None\n",
        "        return None\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = np.zeros((6,7))  # Resetea el tablero, llenándolo nuevamente con ceros\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Agent():\n",
        "    def __init__(self, alpha=0.1, epsilon=0.1):\n",
        "        self.q_values = {}  # Inicializa un diccionario para almacenar los valores Q, donde las claves son pares (estado, acción)\n",
        "        self.alpha = alpha  # Tasa de aprendizaje (alfa)\n",
        "        self.epsilon = epsilon  # Probabilidad de exploración (epsilon)\n",
        "\n",
        "    def choose_action(self, board):\n",
        "        if np.random.rand() < self.epsilon:\n",
        "            return random.choice(board.valid_moves())  # Selección aleatoria de una acción válida\n",
        "        else:\n",
        "            state = str(board.state)  # Convierte el estado actual del tablero a una cadena\n",
        "            actions = board.valid_moves()  # Obtiene las acciones válidas disponibles en el tablero\n",
        "            values = [self.q_values.get((state, action), 0) for action in actions]  # Obtiene los valores Q para cada acción\n",
        "            return actions[np.argmax(values)]  # Elige la acción con el valor Q máximo\n",
        "\n",
        "    def update_q_values(self, state, action, reward, next_state):\n",
        "        # Actualiza los valores Q utilizando la regla de aprendizaje de temporal difference (TD)\n",
        "        current_q_value = self.q_values.get((state, action), 0)  # Obtiene el valor Q actual para el par (estado, acción)\n",
        "        next_max_q_value = max([self.q_values.get((next_state, next_action), 0) for next_action in Board().valid_moves()])  # Obtiene el máximo valor Q para el siguiente estado\n",
        "        new_q_value = current_q_value + self.alpha * (reward + next_max_q_value - current_q_value)  # Calcula el nuevo valor Q\n",
        "        self.q_values[(state, action)] = new_q_value  # Actualiza el valor Q para el par (estado, acción)\n"
      ],
      "metadata": {
        "id": "XmT3LtCTbMpV"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos el tablero y los agentes\n",
        "board = Board()\n",
        "agent1 = Agent(alpha=0.1, epsilon=0.1)\n",
        "agent2 = Agent(alpha=0.1, epsilon=0.1)"
      ],
      "metadata": {
        "id": "fFTdDymafNQW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenamos los agentes jugando entre ellos\n",
        "for _ in tqdm(range(10000)):  # Realizamos 10,000 iteraciones del bucle\n",
        "    board.reset()  # Reseteamos el tablero al estado inicial\n",
        "    state = str(board.state)  # Obtenemos el estado actual del tablero como una cadena\n",
        "    while True:  # Comenzamos un bucle hasta que el juego termine\n",
        "        # Agente 1 elige una acción\n",
        "        action = agent1.choose_action(board)  # Elige una acción basada en la política epsilon-greedy\n",
        "        row, col = board.update(1, action[0], action[1])  # Actualiza el tablero con la acción elegida\n",
        "        next_state = str(board.state)  # Obtiene el próximo estado del tablero como una cadena\n",
        "        reward = board.is_game_over()  # Comprueba si el juego ha terminado y devuelve la recompensa\n",
        "        if reward is not None:  # Si el juego ha terminado\n",
        "            # Actualiza los valores Q de ambos agentes con la recompensa recibida y el próximo estado\n",
        "            agent1.update_q_values(state, action, reward, next_state)\n",
        "            agent2.update_q_values(state, action, reward, next_state)\n",
        "            break  # Sale del bucle\n",
        "        else:  # Si el juego no ha terminado\n",
        "            # Actualiza los valores Q de ambos agentes con una recompensa de 0 para el próximo estado\n",
        "            agent1.update_q_values(state, action, 0, next_state)\n",
        "            agent2.update_q_values(state, action, 0, next_state)\n",
        "        state = next_state  # Actualiza el estado actual del tablero al próximo estado\n",
        "\n",
        "        # Agente 2 elige una acción\n",
        "        action = agent2.choose_action(board)  # Elige una acción basada en la política epsilon-greedy\n",
        "        row, col = board.update(-1, action[0], action[1])  # Actualiza el tablero con la acción elegida\n",
        "        next_state = str(board.state)  # Obtiene el próximo estado del tablero como una cadena\n",
        "        reward = board.is_game_over()  # Comprueba si el juego ha terminado y devuelve la recompensa\n",
        "        if reward is not None:  # Si el juego ha terminado\n",
        "            # Actualiza los valores Q de ambos agentes con la recompensa recibida y el próximo estado\n",
        "            agent1.update_q_values(state, action, reward, next_state)\n",
        "            agent2.update_q_values(state, action, reward, next_state)\n",
        "            break  # Sale del bucle\n",
        "        else:  # Si el juego no ha terminado\n",
        "            # Actualiza los valores Q de ambos agentes con una recompensa de 0 para el próximo estado\n",
        "            agent1.update_q_values(state, action, 0, next_state)\n",
        "            agent2.update_q_values(state, action, 0, next_state)\n",
        "        state = next_state  # Actualiza el estado actual del tablero al próximo estado\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyTS98FJfabh",
        "outputId": "7655a965-b45a-4ad8-f9b2-87d51c05d6e2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10000/10000 [00:31<00:00, 320.44it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluamos los agentes en 10 juegos\n",
        "wins = [0, 0, 0]  # Número de victorias de cada agente y empates\n",
        "for _ in tqdm(range(10)):  # Iteramos sobre 10 juegos\n",
        "    board.reset()  # Reseteamos el tablero al estado inicial\n",
        "    state = str(board.state)  # Obtenemos el estado actual del tablero como una cadena\n",
        "    while True:  # Comenzamos un bucle hasta que el juego termine\n",
        "        # Agente 1 elige una acción\n",
        "        action = agent1.choose_action(board)  # Elige una acción basada en la política epsilon-greedy\n",
        "        row, col = board.update(1, action[0], action[1])  # Actualiza el tablero con la acción elegida\n",
        "        reward = board.is_game_over()  # Comprueba si el juego ha terminado y devuelve la recompensa\n",
        "        if reward is not None:  # Si el juego ha terminado\n",
        "            if reward == 1:  # Si el agente 1 ha ganado\n",
        "                wins[0] += 1  # Incrementa el contador de victorias del agente 1\n",
        "            elif reward == -1:  # Si el agente 2 ha ganado\n",
        "                wins[1] += 1  # Incrementa el contador de victorias del agente 2\n",
        "            else:  # Si hay un empate\n",
        "                wins[2] += 1  # Incrementa el contador de empates\n",
        "            break  # Sale del bucle\n",
        "        state = str(board.state)  # Actualiza el estado actual del tablero como una cadena\n",
        "\n",
        "        # Agente 2 elige una acción\n",
        "        action = agent2.choose_action(board)  # Elige una acción basada en la política epsilon-greedy\n",
        "        row, col = board.update(-1, action[0], action[1])  # Actualiza el tablero con la acción elegida\n",
        "        reward = board.is_game_over()  # Comprueba si el juego ha terminado y devuelve la recompensa\n",
        "        if reward is not None:  # Si el juego ha terminado\n",
        "            if reward == 1:  # Si el agente 1 ha ganado\n",
        "                wins[0] += 1  # Incrementa el contador de victorias del agente 1\n",
        "            elif reward == -1:  # Si el agente 2 ha ganado\n",
        "                wins[1] += 1  # Incrementa el contador de victorias del agente 2\n",
        "            else:  # Si hay un empate\n",
        "                wins[2] += 1  # Incrementa el contador de empates\n",
        "            break  # Sale del bucle\n",
        "        state = str(board.state)  # Actualiza el estado actual del tablero como una cadena\n",
        "\n",
        "# Imprimimos los resultados\n",
        "print(\"Victorias del agente 1:\", wins[0])\n",
        "print(\"Victorias del agente 2:\", wins[1])\n",
        "print(\"Empates:\", wins[2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4Mk_4PHf1yz",
        "outputId": "7efa06fd-06c0-401e-a93b-35a76013fe1f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 198.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Victorias del agente 1: 9\n",
            "Victorias del agente 2: 1\n",
            "Empates: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tKIzqPXhnvUy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}